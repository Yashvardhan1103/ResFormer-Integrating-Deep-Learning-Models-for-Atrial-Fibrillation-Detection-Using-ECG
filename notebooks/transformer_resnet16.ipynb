{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Novel ECG Classification Model: ResNet-16 and Transformer Approach\n",
    "\n",
    "# This notebook implements a novel approach to ECG classification using a combination of ResNet-16 for ECG feature extraction and a Transformer for R-R interval processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yashv\\.conda\\envs\\tf\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # For plotting the confusion matrix\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report  # For evaluation\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "import pywt\n",
    "from wfdb import processing\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, TensorBoard, ReduceLROnPlateau, EarlyStopping\n",
    ")\n",
    "from datetime import datetime\n",
    "\n",
    "from models.resnet import build_resnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in X: 0\n",
      "Data shape after cleaning: (6113, 2000)\n",
      "Labels shape after cleaning: (6113,)\n",
      "Class distribution: Counter({0: 5076, 1: 758, 3: 279})\n",
      "Data types of X after cleaning: float64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset_path = 'data/raw/physionet2017.csv'  # Update with your dataset path\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Extract ECG signals (features) and labels\n",
    "X = data.drop(['name', 'label'], axis=1)  # Drop 'name' and 'label' columns\n",
    "y = data['label'].values  # Use 'label' as the labels\n",
    "\n",
    "# Convert X to a NumPy array\n",
    "X = X.values\n",
    "\n",
    "# Remove Class-2 (\"Other\") entries\n",
    "mask = y != 2\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# Convert X to numeric, replacing non-numeric values with NaN\n",
    "X = pd.DataFrame(X)\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Check for NaN values\n",
    "print(\"Number of NaN values in X:\", X.isna().sum().sum())\n",
    "\n",
    "# Fill NaN values with the mean of each column\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "# Convert back to NumPy array\n",
    "X = X.values\n",
    "\n",
    "# Check the shapes and data types\n",
    "print(\"Data shape after cleaning:\", X.shape)\n",
    "print(\"Labels shape after cleaning:\", y.shape)\n",
    "print(\"Class distribution:\", Counter(y))\n",
    "print(\"Data types of X after cleaning:\", X.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Noise Removal and Signal Detrending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data shape: (6113, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Noise Removal and Signal Detrending\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "fs = 300  # Sampling frequency in Hz\n",
    "lowcut = 0.5\n",
    "highcut = 45.0\n",
    "\n",
    "# Apply the filter to each ECG signal\n",
    "X_filtered = np.array([butter_bandpass_filter(ecg, lowcut, highcut, fs) for ecg in X])\n",
    "\n",
    "print(\"Filtered data shape:\", X_filtered.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. R-Peak Detection and Calculation of R-R Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example R-R intervals: [0.74333333 0.71666667 0.70666667 0.73       0.76       0.76666667]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: R-Peak Detection and Calculation of R-R Intervals\n",
    "def calculate_r_r_intervals(ecg_signal, fs=300):\n",
    "    r_peaks = processing.gqrs_detect(sig=ecg_signal, fs=fs)\n",
    "    r_r_intervals = np.diff(r_peaks) / fs  # Calculate intervals between R-peaks\n",
    "    return r_r_intervals\n",
    "\n",
    "# Calculate R-R intervals for each ECG signal\n",
    "X_r_r_intervals = [calculate_r_r_intervals(ecg) for ecg in X_filtered]\n",
    "\n",
    "# Example of R-R intervals\n",
    "print(\"Example R-R intervals:\", X_r_r_intervals[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction Using Wavelet Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wavelet features shape: (6113, 16)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Feature Extraction\n",
    "def extract_wavelet_features(segment, wavelet='db1', level=3):\n",
    "    coeffs = pywt.wavedec(segment, wavelet, level=level)\n",
    "    features = []\n",
    "    for coeff in coeffs:\n",
    "        features.extend([\n",
    "            np.mean(coeff),\n",
    "            np.std(coeff),\n",
    "            np.max(coeff),\n",
    "            np.min(coeff)\n",
    "        ])\n",
    "    return np.array(features)\n",
    "\n",
    "# Extract features for each ECG signal\n",
    "X_pqrst_features = [extract_wavelet_features(ecg) for ecg in X_filtered]\n",
    "X_pqrst_features = np.array(X_pqrst_features)\n",
    "\n",
    "print(\"Wavelet features shape:\", X_pqrst_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Padding and Normalizing R-R Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded R-R intervals shape: (6113, 17)\n",
      "Combined features shape: (6113, 33)\n"
     ]
    }
   ],
   "source": [
    "# Pad R-R intervals to a fixed length\n",
    "max_r_r_length = max(len(intervals) for intervals in X_r_r_intervals)\n",
    "X_r_r_padded = np.array([\n",
    "    np.pad(intervals, (0, max_r_r_length - len(intervals)), 'constant')\n",
    "    for intervals in X_r_r_intervals\n",
    "])\n",
    "\n",
    "print(\"Padded R-R intervals shape:\", X_r_r_padded.shape)\n",
    "\n",
    "# Normalize features\n",
    "scaler_ecg = StandardScaler()\n",
    "X_pqrst_normalized = scaler_ecg.fit_transform(X_pqrst_features)\n",
    "\n",
    "scaler_rr = StandardScaler()\n",
    "X_r_r_normalized = scaler_rr.fit_transform(X_r_r_padded)\n",
    "\n",
    "# Combine normalized features\n",
    "X_combined = np.hstack((X_pqrst_normalized, X_r_r_normalized))\n",
    "\n",
    "print(\"Combined features shape:\", X_combined.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (4890, 33)\n",
      "Validation set shape: (611, 33)\n",
      "Test set shape: (612, 33)\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_combined, y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_rr_train, X_rr_temp = train_test_split(\n",
    "    X_r_r_padded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "X_rr_val, X_rr_test = train_test_split(\n",
    "    X_rr_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Class Balancing with SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled data shape: (12168, 33)\n",
      "Resampled labels distribution: Counter({3: 4058, 1: 4056, 0: 4054})\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Class Balancing with SMOTETomek\n",
    "X_combined_train = np.hstack((X_train, X_rr_train))\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "\n",
    "X_combined_resampled, y_resampled = smote_tomek.fit_resample(X_combined_train, y_train)\n",
    "\n",
    "# Split the resampled data back into ECG and R-R interval features\n",
    "num_ecg_features = X_train.shape[1]\n",
    "X_resampled = X_combined_resampled[:, :num_ecg_features]\n",
    "X_rr_resampled = X_combined_resampled[:, num_ecg_features:]\n",
    "\n",
    "print(\"Resampled data shape:\", X_resampled.shape)\n",
    "print(\"Resampled labels distribution:\", Counter(y_resampled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reshape Data for Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped ECG features shape: (12168, 33, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape ECG features for CNN input\n",
    "X_resampled = X_resampled.reshape(-1, X_resampled.shape[1], 1)\n",
    "X_train = X_train.reshape(-1, X_train.shape[1], 1)\n",
    "X_val = X_val.reshape(-1, X_val.shape[1], 1)\n",
    "X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
    "\n",
    "print(\"Reshaped ECG features shape:\", X_resampled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Define the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ECG_Classification_Model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " ECG_Input (InputLayer)         [(None, 33, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " CustomResNet (Functional)      (None, 128)          522048      ['ECG_Input[0][0]']              \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 128)          16512       ['CustomResNet[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)           (None, 128)          0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 128)          16512       ['dropout_27[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)           (None, 128)          0           ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 128)          0           ['dropout_28[0][0]',             \n",
      "                                                                  'dropout_27[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_6 (Reshape)            (None, 1, 128)       0           ['add_21[0][0]']                 \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 1, 128)      263808      ['reshape_6[0][0]',              \n",
      " eadAttention)                                                    'reshape_6[0][0]']              \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, 1, 128)       0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'reshape_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 1, 128)      256         ['add_22[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 1, 256)       33024       ['layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)           (None, 1, 256)       0           ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 1, 128)       32896       ['dropout_29[0][0]']             \n",
      "                                                                                                  \n",
      " RR_Interval_Input (InputLayer)  [(None, 17)]        0           []                               \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 1, 128)       0           ['dense_27[0][0]',               \n",
      "                                                                  'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 128)          2304        ['RR_Interval_Input[0][0]']      \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 1, 128)      256         ['add_23[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " reshape_7 (Reshape)            (None, 1, 128)       0           ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 128)          0           ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)            (None, 128)          0           ['reshape_7[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 256)          0           ['flatten_6[0][0]',              \n",
      "                                                                  'flatten_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 128)          32896       ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)           (None, 128)          0           ['dense_29[0][0]']               \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 64)           8256        ['dropout_30[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)           (None, 64)           0           ['dense_30[0][0]']               \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 3)            195         ['dropout_31[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 928,963\n",
      "Trainable params: 927,619\n",
      "Non-trainable params: 1,344\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Custom ResNet block\n",
    "def resnet_block(input_data, filters, kernel_size, stride=1, dropout_rate=0.3, l2_lambda=0.001):\n",
    "    x = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=stride, padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(l2_lambda))(input_data)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=1, padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(l2_lambda))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    shortcut = layers.Conv1D(filters=filters, kernel_size=1, strides=stride, padding='same',\n",
    "                             kernel_regularizer=regularizers.l2(l2_lambda))(input_data)\n",
    "    shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "# Build custom ResNet model\n",
    "def build_custom_resnet(input_shape, dropout_rate=0.3, l2_lambda=0.001):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = resnet_block(inputs, filters=32, kernel_size=16, stride=1, dropout_rate=dropout_rate, l2_lambda=l2_lambda)\n",
    "    x = resnet_block(x, filters=64, kernel_size=16, stride=2, dropout_rate=dropout_rate, l2_lambda=l2_lambda)\n",
    "    x = resnet_block(x, filters=128, kernel_size=16, stride=2, dropout_rate=dropout_rate, l2_lambda=l2_lambda)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    model = models.Model(inputs, x, name=\"CustomResNet\")\n",
    "    return model\n",
    "\n",
    "# Chain of Thought block\n",
    "def chain_of_thought_block(input_data, units=128):\n",
    "    x = layers.Dense(units, activation='relu')(input_data)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # Residual connection\n",
    "    x_res = layers.Dense(units, activation='relu')(x)\n",
    "    x_res = layers.Dropout(0.3)(x_res)\n",
    "\n",
    "    x = layers.Add()([x_res, x])\n",
    "    return x\n",
    "\n",
    "# Multi-Head Attention block\n",
    "def multi_head_attention_block(input_data, head_size, num_heads, ff_dim, dropout=0.3):\n",
    "    x = layers.Reshape((1, -1))(input_data)  # Reshape to 3D tensor for attention\n",
    "    x_att = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = layers.Add()([x_att, x])\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    x_ff = layers.Dense(ff_dim, activation='relu')(x)\n",
    "    x_ff = layers.Dropout(dropout)(x_ff)\n",
    "    x_ff = layers.Dense(input_data.shape[-1])(x_ff)\n",
    "    x = layers.Add()([x_ff, x])\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)  # Flatten back to 2D tensor\n",
    "    return x\n",
    "\n",
    "# Build the final model\n",
    "def build_final_model(input_shape_ecg, input_shape_rr, num_classes, head_size=128, num_heads=4, ff_dim=256, dropout=0.3, dropout_rate=0.3):\n",
    "    # Input Layers\n",
    "    input_ecg = layers.Input(shape=input_shape_ecg, name=\"ECG_Input\")\n",
    "    input_rr = layers.Input(shape=input_shape_rr, name=\"RR_Interval_Input\")\n",
    "\n",
    "    # Custom ResNet for ECG feature extraction\n",
    "    resnet_model = build_custom_resnet(input_shape_ecg, dropout_rate=dropout_rate)\n",
    "    resnet_features = resnet_model(input_ecg)\n",
    "\n",
    "    # Chain of Thought Mechanism on ResNet features\n",
    "    cot_output = chain_of_thought_block(resnet_features, units=128)\n",
    "\n",
    "    # Multi-Head Attention Block with Self-Attention on ResNet features\n",
    "    attention_output = multi_head_attention_block(cot_output, head_size=head_size, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout)\n",
    "\n",
    "    # Processing R-R Interval Features\n",
    "    rr_dense = layers.Dense(128, activation=\"relu\")(input_rr)\n",
    "    rr_dense = layers.Reshape((1, 128))(rr_dense)\n",
    "\n",
    "    # Combine ResNet features (ECG) and RR Interval features\n",
    "    combined_features = layers.Concatenate()([attention_output, layers.Flatten()(rr_dense)])\n",
    "\n",
    "    # Fully Connected Layers for Final Classification\n",
    "    x = layers.Dense(128, activation='relu')(combined_features)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Final model with two inputs\n",
    "    model = models.Model(inputs=[input_ecg, input_rr], outputs=output, name=\"ECG_Classification_Model\")\n",
    "    return model\n",
    "\n",
    "# Prepare your data (make sure to replace with your actual data variables)\n",
    "# Ensure that X_resampled and X_val are reshaped to (num_samples, sequence_length, 1)\n",
    "# For example:\n",
    "# X_resampled = X_resampled.reshape(-1, X_resampled.shape[1], 1)\n",
    "# X_val = X_val.reshape(-1, X_val.shape[1], 1)\n",
    "\n",
    "# Define input shapes for ECG and R-R intervals\n",
    "input_shape_ecg = (X_resampled.shape[1], 1)  # ECG data shape\n",
    "input_shape_rr = (X_rr_resampled.shape[1],)  # R-R interval data shape\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = len(np.unique(y_resampled))\n",
    "\n",
    "# Build and compile the final model\n",
    "model = build_final_model(input_shape_ecg, input_shape_rr, num_classes)\n",
    "\n",
    "# Compile with gradient clipping\n",
    "optimizer = Adam(learning_rate=1e-6, clipnorm=1.0)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Set Up Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "log_dir = f\"logs/tensorboard/Novel_Model/{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='novel_model_best.h5', save_best_only=True, verbose=1\n",
    ")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=3, min_lr=1e-8\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    checkpoint_callback,\n",
    "    tensorboard_callback,\n",
    "    lr_scheduler,\n",
    "    early_stopping\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "379/381 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.3331\n",
      "Epoch 1: val_loss did not improve from inf\n",
      "381/381 [==============================] - 11s 22ms/step - loss: nan - accuracy: 0.3328 - val_loss: nan - val_accuracy: 0.8298 - lr: 1.0000e-06\n",
      "Epoch 2/100\n",
      "381/381 [==============================] - ETA: 0s - loss: nan - accuracy: 0.3332\n",
      "Epoch 2: val_loss did not improve from inf\n",
      "381/381 [==============================] - 11s 29ms/step - loss: nan - accuracy: 0.3332 - val_loss: nan - val_accuracy: 0.8298 - lr: 1.0000e-06\n",
      "Epoch 3/100\n",
      "299/381 [======================>.......] - ETA: 2s - loss: nan - accuracy: 0.3282"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Model Training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_rr_resampled\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_resampled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_rr_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yashv\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\yashv\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\yashv\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\yashv\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\yashv\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\yashv\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yashv\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\yashv\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\yashv\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "history = model.fit(\n",
    "    [X_resampled, X_rr_resampled], y_resampled,\n",
    "    validation_data=([X_val, X_rr_val], y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "evaluation = model.evaluate([X_test, X_rr_test], y_test)\n",
    "print(f\"Test Loss: {evaluation[0]:.4f}\")\n",
    "print(f\"Test Accuracy: {evaluation[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Make Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "y_pred_probs = model.predict([X_test, X_rr_test])\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "\n",
    "# Labeling the axes\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in X_resampled: False\n",
      "Infs in X_resampled: False\n",
      "NaNs in X_rr_resampled: False\n",
      "Infs in X_rr_resampled: False\n",
      "NaNs in y_resampled: False\n",
      "Infs in y_resampled: False\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN and Inf in ECG data\n",
    "print(\"NaNs in X_resampled:\", np.isnan(X_resampled).any())\n",
    "print(\"Infs in X_resampled:\", np.isinf(X_resampled).any())\n",
    "\n",
    "# Check for NaN and Inf in RR interval data\n",
    "print(\"NaNs in X_rr_resampled:\", np.isnan(X_rr_resampled).any())\n",
    "print(\"Infs in X_rr_resampled:\", np.isinf(X_rr_resampled).any())\n",
    "\n",
    "# Check for NaN and Inf in labels\n",
    "print(\"NaNs in y_resampled:\", np.isnan(y_resampled).any())\n",
    "print(\"Infs in y_resampled:\", np.isinf(y_resampled).any())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
